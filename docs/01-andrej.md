# 2025 LLM 한 해 돌아보기

> 원문: [Andrej Karpathy - 2025 LLM Year in Review](https://x.com/karpathy/status/2002118205729562949)
> https://karpathy.bearblog.dev/year-in-review-2025/
> 작성일: 2025년 12월 20일

2025년은 LLM이 정말 빠르게 발전한 해였다. 아래는 내가 보기에 특히 주목할 만하고, 약간 놀라웠던 "패러다임 전환"들이다 - 판도를 바꾸고 개념적으로 눈에 띄었던 것들.

---

## 1. RLVR (검증 가능한 보상 기반 강화학습)

2025년 초까지, 모든 AI 연구소의 LLM 학습 파이프라인은 대충 이랬다:

1. **사전학습 (Pretraining)** - GPT-2/3 시절, 2020년경
2. **지도 미세조정 (SFT)** - InstructGPT, 2022년경
3. **인간 피드백 강화학습 (RLHF)** - 2022년경

이게 한동안 검증된 레시피였다. 그런데 2025년에 **RLVR**이 새로운 핵심 단계로 떠올랐다.

핵심 아이디어는 이렇다: 수학 문제나 코딩 퍼즐처럼 **정답을 자동으로 검증할 수 있는 환경**에서 LLM을 학습시키면, LLM이 스스로 "추론"처럼 보이는 전략을 개발한다. 문제를 단계별로 쪼개고, 이리저리 시도하면서 답을 찾아가는 방식을 터득하는 것이다 (DeepSeek R1 논문 참조). 이건 기존 방식으로는 가르치기 어려웠다 - 최적의 사고 과정이 뭔지 우리도 모르니까. LLM이 보상을 최적화하면서 자기한테 맞는 방법을 알아서 찾아야 한다.

SFT와 RLHF는 비교적 가벼운 미세조정인 반면, RLVR은 객관적이고 조작 불가능한 보상 함수를 쓰기 때문에 훨씬 오래 학습할 수 있다. 그리고 성능 대비 비용 효율이 좋다는 게 밝혀지면서, 원래 사전학습에 쓰려던 컴퓨팅 자원을 RLVR이 다 빨아들였다. 그래서 2025년 대부분의 성능 향상은 모델 크기를 키운 게 아니라, 비슷한 크기의 모델을 훨씬 더 오래 RL 돌린 결과였다.

또 하나 새로운 건, 추론 시간에 "더 오래 생각하게" 해서 성능을 올릴 수 있는 새로운 손잡이가 생겼다는 것이다 (테스트 타임 컴퓨팅). OpenAI o1(2024년 말)이 첫 RLVR 모델이었지만, o3(2025년 초)가 진짜 체감되는 변곡점이었다.

---

## 2. 유령 vs 동물 / 들쭉날쭉한 지능

2025년은 나를 포함해 업계 전체가 LLM 지능의 "모양"을 직관적으로 이해하기 시작한 해다.

우리는 **동물을 진화시키는 게 아니라, 유령을 소환하고 있다.**

생각해보면 당연하다. LLM의 모든 게 다르다 - 아키텍처, 학습 데이터, 알고리즘, 특히 최적화 압력. 인간의 뇌는 정글에서 부족이 살아남도록 최적화됐지만, LLM은 인류의 텍스트를 흉내내고, 수학 퍼즐 점수를 올리고, LM Arena에서 업보트 받도록 최적화됐다. 완전히 다른 압력을 받으면서 완전히 다른 형태의 지능이 나온 것이다.

그래서 LLM은 검증 가능한 영역(수학, 코딩) 근처에서는 성능이 뾰족하게 튀어나오고, 전체적으로 **재밌을 정도로 들쭉날쭉한** 성능을 보인다. 동시에 천재적인 박학다식과 멍청한 초등학생이 공존하고, 탈옥 공격 한 번에 데이터를 유출당할 수도 있다.

> (이미지: 인간 지능 파란색, AI 지능 빨간색. 인간 지능도 나름대로 들쭉날쭉하다는 걸 보여주는 밈 - 원본 출처를 잃어버렸다)

이거랑 관련해서, 2025년에 벤치마크에 대한 신뢰가 많이 떨어졌다. 벤치마크는 거의 정의상 "검증 가능한 환경"이라서 RLVR로 쉽게 공략당한다. 연구소들이 벤치마크 근처의 임베딩 공간에 인접한 환경을 만들어서 거기에 맞게 성능을 "뾰족하게" 키운다. 테스트셋으로 학습하는 게 하나의 예술이 됐다.

**모든 벤치마크를 다 이기면서도 AGI는 아닌 게 어떤 모습일까?**

---

## 3. Cursor / LLM 앱이라는 새로운 레이어

Cursor의 폭발적 성장 외에 내가 주목한 건, 이게 **"LLM 앱"이라는 새로운 레이어**를 제대로 보여줬다는 것이다. 사람들이 "X 분야의 Cursor"를 말하기 시작했다.

올해 YC 강연에서도 얘기했지만, Cursor 같은 LLM 앱들은 특정 분야에 맞춰 LLM 호출을 묶고 조율한다:

- **컨텍스트 엔지니어링**을 한다
- 여러 LLM 호출을 복잡한 DAG로 엮어서 성능과 비용 균형을 맞춘다
- 사람이 개입할 수 있는 전용 UI를 제공한다
- **자율성 슬라이더**를 제공한다

2025년 내내 이 앱 레이어가 얼마나 "두꺼운지" 논쟁이 있었다. LLM 연구소들이 다 먹을 건가, 아니면 LLM 앱들이 설 자리가 있는가? 내 생각엔, 연구소들은 범용적으로 유능한 "대졸 신입"을 배출하고, LLM 앱들은 그들에게 비공개 데이터, 센서, 피드백 루프를 공급해서 특정 분야의 "배치된 전문가"로 만드는 역할을 할 것 같다.

---

## 4. Claude Code / 내 컴퓨터에 사는 AI

Claude Code(CC)는 **LLM 에이전트가 뭔지 처음으로 제대로 보여준 사례**다 - 도구 사용과 추론을 루프로 엮어서 복잡한 문제를 풀어나가는 것.

그리고 CC의 핵심은 **내 컴퓨터에서, 내 환경과 데이터와 컨텍스트로 돌아간다**는 것이다.

OpenAI는 이걸 잘못 짚었다고 본다. 초기 Codex/에이전트를 ChatGPT에서 오케스트레이션하는 클라우드 컨테이너에 집중했는데, 클라우드의 에이전트 떼가 "AGI 최종 형태"처럼 느껴질 수 있지만, 지금은 들쭉날쭉한 성능의 과도기다. 개발자 컴퓨터에서 직접 돌리는 게 더 맞다.

핵심은 AI 연산이 어디서 도느냐가 아니다. 진짜 중요한 건 **이미 부팅된 내 컴퓨터, 내 설치 환경, 내 데이터, 내 시크릿, 내 설정, 그리고 빠른 상호작용**이다. Anthropic은 이 우선순위를 제대로 잡고, CC를 심플한 CLI로 패키징했다.

구글처럼 "가는" 웹사이트가 아니라, 내 컴퓨터에 **"사는"** 작은 유령. AI와 상호작용하는 완전히 새로운 패러다임이다.

---

## 5. 바이브 코딩

2025년은 AI가 **영어만으로 웬만한 프로그램을 다 만들 수 있는** 성능 임계점을 넘은 해다. 코드가 있다는 걸 잊어도 될 정도로.

재밌는 건, 내가 샤워하다가 떠올린 "바이브 코딩"이라는 용어가 이렇게 퍼질 줄 몰랐다는 거다 :)

바이브 코딩으로 프로그래밍은 더 이상 고도로 훈련된 전문가만의 영역이 아니게 됐다. 누구나 할 수 있다. 예전에 쓴 "Power to the people" 글처럼, (다른 모든 기술과 달리) LLM은 전문가나 기업이나 정부보다 **일반인에게 더 큰 혜택**을 준다.

하지만 바이브 코딩은 일반인만 돕는 게 아니다. 전문가들도 **안 그러면 절대 안 만들었을 소프트웨어**를 훨씬 많이 만들게 된다. 나는 nanochat에서 Rust로 고성능 BPE 토크나이저를 바이브 코딩했다 - Rust를 제대로 배우거나 기존 라이브러리 쓰는 대신. 올해 빠른 데모용으로 많은 프로젝트를 바이브 코딩했다 (menugen, llm-council, reader3, HN time capsule 등). 버그 하나 찾으려고 임시 앱 전체를 바이브 코딩하기도 했다, 왜 안 되겠어 - 코드가 갑자기 **공짜고, 일회용이고, 쓰고 버리는 것**이 됐다.

바이브 코딩은 소프트웨어의 지형을 바꾸고, 직업의 정의를 바꿀 것이다.

---

## 6. 나노 바나나 / LLM GUI

Google Gemini Nano Banana는 2025년 가장 놀라운 패러다임 전환 모델 중 하나다.

내 세계관에서 LLM은 70-80년대 컴퓨터와 비슷한, **다음 주요 컴퓨팅 패러다임**이다. 그래서 비슷한 이유로 비슷한 혁신들이 나올 것이다 - 퍼스널 컴퓨팅, 마이크로컨트롤러(인지 코어), 인터넷(에이전트들의) 같은 것들의 등가물.

특히 UI/UX 측면에서, LLM과 "채팅"하는 건 **80년대 콘솔에 명령어 치는 것**과 비슷하다. 텍스트는 컴퓨터(와 LLM)가 좋아하는 형식이지만, 사람은 그렇지 않다. 사람은 텍스트 읽기 싫어한다 - 느리고 힘들다. 대신 시각적으로, 공간적으로 정보를 소비하고 싶어한다. 그래서 전통적 컴퓨팅에서 GUI가 발명된 것이다.

마찬가지로 LLM도 **우리가 선호하는 형식**으로 말해야 한다 - 이미지, 인포그래픽, 슬라이드, 화이트보드, 애니메이션, 웹앱 등. 지금의 이모지나 마크다운은 그 초기 형태일 뿐이다.

그럼 **LLM GUI**를 누가 만들 건가? Nano Banana가 그 첫 힌트다. 중요한 건 단순히 이미지 생성이 아니라, **텍스트 생성 + 이미지 생성 + 세계 지식**이 모델 가중치 안에서 얽혀서 나오는 통합된 능력이라는 점이다.

---

## 요약

2025년은 LLM에게 흥미롭고 살짝 놀라운 해였다. LLM은 새로운 종류의 지능으로 떠오르고 있다 - 내 예상보다 훨씬 똑똑하면서 동시에 훨씬 멍청하다. 어쨌든 엄청나게 유용하고, 업계는 현재 성능에서조차 잠재력의 10%도 못 뽑고 있다고 본다.

시도할 아이디어가 너무 많고, 개념적으로 이 분야는 완전히 열려 있다. 올해 초 Dwarkesh 팟캐스트에서 말했듯이, 나는 (겉보기엔 모순적이지만) 빠른 발전이 계속될 것이면서 동시에 할 일이 아직 산더미라고 믿는다.

안전벨트 매라.
